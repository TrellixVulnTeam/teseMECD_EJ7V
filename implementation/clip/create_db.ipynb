{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf97a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "from transformers import CLIPTokenizer\n",
    "from transformers import CLIPFeatureExtractor\n",
    "from transformers import CLIPProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a58c769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../e-ViL/data/esnlive_train.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "text = dataset.iloc[0]['hypothesis']\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64edf29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [49406, 1237, 896, 1656, 47569, 320, 17859, 2183, 269, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [49406, 1237, 896, 1656, 47569, 320, 17859, 2183, 269, 49407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "10\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "out1 = tokenizer(text)\n",
    "\n",
    "out2 = tokenizer(text,padding=\"max_length\",\n",
    "            max_length=68,\n",
    "            truncation=True)\n",
    "print(out1)\n",
    "print(out2)\n",
    "print(len(out1['input_ids']))\n",
    "print(len(out2['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed356823",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_extract = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "img_path = '../e-ViL/data/flickr30k_images/flickr30k_images/178045.jpg'\n",
    "img = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95f915ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-0.5659947 , -0.87256163, -0.8433648 , ..., -0.4638057 ,\n",
       "          -0.50760096, -0.43460885],\n",
       "         [-0.6389868 , -0.9309554 , -1.0769397 , ..., -0.37621516,\n",
       "          -0.4638057 , -0.31782144],\n",
       "         [-0.7995695 , -0.96015227, -1.1645303 , ..., -0.405412  ,\n",
       "          -0.50760096, -0.2740262 ],\n",
       "         ...,\n",
       "         [-0.14264035, -0.11344349, -0.06964822, ..., -0.2740262 ,\n",
       "           0.03254076,  0.12013142],\n",
       "         [-0.14264035, -0.08424664, -0.04045137, ..., -0.25942776,\n",
       "          -0.11344349, -0.04045137],\n",
       "         [-0.12804192, -0.0550498 , -0.02585294, ..., -0.21563247,\n",
       "          -0.2302309 , -0.2740262 ]],\n",
       " \n",
       "        [[-0.32635912, -0.6415222 , -0.6115067 , ..., -0.37138242,\n",
       "          -0.43141347, -0.32635912],\n",
       "         [-0.37138242, -0.6865455 , -0.83662325, ..., -0.2813358 ,\n",
       "          -0.38639018, -0.22130474],\n",
       "         [-0.5064523 , -0.6865455 , -0.8966543 , ..., -0.29634356,\n",
       "          -0.4164057 , -0.16127367],\n",
       "         ...,\n",
       "         [ 0.01881953,  0.0338273 ,  0.06384283, ..., -0.07122707,\n",
       "           0.24393615,  0.27395168],\n",
       "         [ 0.0338273 ,  0.04883507,  0.06384283, ...,  0.01881953,\n",
       "           0.19891284,  0.24393615],\n",
       "         [ 0.0338273 ,  0.04883507,  0.06384283, ...,  0.13888167,\n",
       "           0.13888167,  0.1238739 ]],\n",
       " \n",
       "        [[-0.21463387, -0.51325524, -0.48481512, ..., -0.2857342 ,\n",
       "          -0.32839438, -0.29995427],\n",
       "         [-0.27151415, -0.5559154 , -0.6981161 , ..., -0.20041381,\n",
       "          -0.31417432, -0.20041381],\n",
       "         [-0.39949474, -0.57013553, -0.7692165 , ..., -0.21463387,\n",
       "          -0.35683453, -0.15775362],\n",
       "         ...,\n",
       "         [ 0.56746984,  0.56746984,  0.59590995, ...,  0.36838892,\n",
       "           0.7381106 ,  0.6527902 ],\n",
       "         [ 0.5816899 ,  0.59590995,  0.61013   , ...,  0.33994877,\n",
       "           0.5532498 ,  0.5816899 ],\n",
       "         [ 0.5816899 ,  0.6243501 ,  0.63857013, ...,  0.36838892,\n",
       "           0.36838892,  0.4110491 ]]], dtype=float32)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = features_extract(img)\n",
    "feats['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50868841",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7cd11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[49406,  1237,   896,  1656, 47569,   320, 17859,  2183,   269, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[[[-0.5660, -0.8726, -0.8434,  ..., -0.4638, -0.5076, -0.4346],\n",
      "          [-0.6390, -0.9310, -1.0769,  ..., -0.3762, -0.4638, -0.3178],\n",
      "          [-0.7996, -0.9602, -1.1645,  ..., -0.4054, -0.5076, -0.2740],\n",
      "          ...,\n",
      "          [-0.1426, -0.1134, -0.0696,  ..., -0.2740,  0.0325,  0.1201],\n",
      "          [-0.1426, -0.0842, -0.0405,  ..., -0.2594, -0.1134, -0.0405],\n",
      "          [-0.1280, -0.0550, -0.0259,  ..., -0.2156, -0.2302, -0.2740]],\n",
      "\n",
      "         [[-0.3264, -0.6415, -0.6115,  ..., -0.3714, -0.4314, -0.3264],\n",
      "          [-0.3714, -0.6865, -0.8366,  ..., -0.2813, -0.3864, -0.2213],\n",
      "          [-0.5065, -0.6865, -0.8967,  ..., -0.2963, -0.4164, -0.1613],\n",
      "          ...,\n",
      "          [ 0.0188,  0.0338,  0.0638,  ..., -0.0712,  0.2439,  0.2740],\n",
      "          [ 0.0338,  0.0488,  0.0638,  ...,  0.0188,  0.1989,  0.2439],\n",
      "          [ 0.0338,  0.0488,  0.0638,  ...,  0.1389,  0.1389,  0.1239]],\n",
      "\n",
      "         [[-0.2146, -0.5133, -0.4848,  ..., -0.2857, -0.3284, -0.3000],\n",
      "          [-0.2715, -0.5559, -0.6981,  ..., -0.2004, -0.3142, -0.2004],\n",
      "          [-0.3995, -0.5701, -0.7692,  ..., -0.2146, -0.3568, -0.1578],\n",
      "          ...,\n",
      "          [ 0.5675,  0.5675,  0.5959,  ...,  0.3684,  0.7381,  0.6528],\n",
      "          [ 0.5817,  0.5959,  0.6101,  ...,  0.3399,  0.5532,  0.5817],\n",
      "          [ 0.5817,  0.6244,  0.6386,  ...,  0.3684,  0.3684,  0.4110]]]])}\n",
      "{'input_ids': tensor([[49406,  1237,   896,  1656, 47569,   320, 17859,  2183,   269, 49407,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'pixel_values': tensor([[[[-0.5660, -0.8726, -0.8434,  ..., -0.4638, -0.5076, -0.4346],\n",
      "          [-0.6390, -0.9310, -1.0769,  ..., -0.3762, -0.4638, -0.3178],\n",
      "          [-0.7996, -0.9602, -1.1645,  ..., -0.4054, -0.5076, -0.2740],\n",
      "          ...,\n",
      "          [-0.1426, -0.1134, -0.0696,  ..., -0.2740,  0.0325,  0.1201],\n",
      "          [-0.1426, -0.0842, -0.0405,  ..., -0.2594, -0.1134, -0.0405],\n",
      "          [-0.1280, -0.0550, -0.0259,  ..., -0.2156, -0.2302, -0.2740]],\n",
      "\n",
      "         [[-0.3264, -0.6415, -0.6115,  ..., -0.3714, -0.4314, -0.3264],\n",
      "          [-0.3714, -0.6865, -0.8366,  ..., -0.2813, -0.3864, -0.2213],\n",
      "          [-0.5065, -0.6865, -0.8967,  ..., -0.2963, -0.4164, -0.1613],\n",
      "          ...,\n",
      "          [ 0.0188,  0.0338,  0.0638,  ..., -0.0712,  0.2439,  0.2740],\n",
      "          [ 0.0338,  0.0488,  0.0638,  ...,  0.0188,  0.1989,  0.2439],\n",
      "          [ 0.0338,  0.0488,  0.0638,  ...,  0.1389,  0.1389,  0.1239]],\n",
      "\n",
      "         [[-0.2146, -0.5133, -0.4848,  ..., -0.2857, -0.3284, -0.3000],\n",
      "          [-0.2715, -0.5559, -0.6981,  ..., -0.2004, -0.3142, -0.2004],\n",
      "          [-0.3995, -0.5701, -0.7692,  ..., -0.2146, -0.3568, -0.1578],\n",
      "          ...,\n",
      "          [ 0.5675,  0.5675,  0.5959,  ...,  0.3684,  0.7381,  0.6528],\n",
      "          [ 0.5817,  0.5959,  0.6101,  ...,  0.3399,  0.5532,  0.5817],\n",
      "          [ 0.5817,  0.6244,  0.6386,  ...,  0.3684,  0.3684,  0.4110]]]])}\n",
      "OUT1\n",
      "key:  input_ids  shape: torch.Size([1, 10])\n",
      "key:  attention_mask  shape: torch.Size([1, 10])\n",
      "key:  pixel_values  shape: torch.Size([1, 3, 224, 224])\n",
      "OUT2\n",
      "key:  input_ids  shape: torch.Size([1, 68])\n",
      "key:  attention_mask  shape: torch.Size([1, 68])\n",
      "key:  pixel_values  shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#processed_batch = processor(text=batch[\"question\"], images=imgs_to_encode, padding=True, return_tensors=\"pt\")\n",
    "out1 = processor(text=text, images=img, padding=True, return_tensors=\"pt\")\n",
    "out2 = processor(text=text, images=img, return_tensors=\"pt\",padding=\"max_length\", max_length=68,truncation=True)\n",
    "print(out1)\n",
    "print(out2)\n",
    "print('OUT1')\n",
    "for key in out1:\n",
    "    print('key: ',key,' shape:',out1[key].shape)\n",
    "print('OUT2')\n",
    "for key in out2:\n",
    "    print('key: ',key,' shape:',out2[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(lmdb_file_name=\"./data/my_image_db\"):\n",
    "    lmdb_env = lmdb.open(lmdb_file_name, readonly=True)\n",
    "    stats = lmdb_env.stat()\n",
    "    info = lmdb_env.info()\n",
    "    lmdb_env.close()\n",
    "    dbSize = stats['psize'] * (stats['leaf_pages'] + stats['branch_pages'] + stats['overflow_pages'])\n",
    "    return dbSize/1024/1024\n",
    "getSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDB():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feature_extract = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        data_path = '../e-ViL/data/'\n",
    "        images_path ='flickr30k_images/flickr30k_images/'\n",
    "        self.path = data_path+images_path\n",
    "        self.images = [f for f in os.listdir(self.path)]\n",
    "        self.images.sort()\n",
    "        return\n",
    "    \n",
    "    def get_visual_features(self,img):\n",
    "        return self.feature_extract(img)\n",
    "    \n",
    "    def write_to_lmdb(self,filename,map_size = 1000000000):#1GB\n",
    "        k = 0\n",
    "        env = lmdb.open(filename, map_size= map_size)\n",
    "        for idx in range(len(img_list)):\n",
    "            print(k)\n",
    "            k+=1\n",
    "            txn = env.begin(write=True)\n",
    "            img = self.images[idx]\n",
    "            img_path = self.path+img\n",
    "            \n",
    "            features = self.get_visual_features(img_path)\n",
    "            \n",
    "            item = {'features': features.numpy()}\n",
    "            txn.put(key = img.encode(), value = pickle.dumps(item))\n",
    "            # Commit changes through the commit() function \n",
    "            txn.commit()\n",
    "            if(k%1000==0):\n",
    "                torch.cuda.empty_cache()\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
